{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-07T04:16:10.211198Z",
     "iopub.status.busy": "2022-06-07T04:16:10.210810Z",
     "iopub.status.idle": "2022-06-07T04:17:18.914917Z",
     "shell.execute_reply": "2022-06-07T04:17:18.913801Z",
     "shell.execute_reply.started": "2022-06-07T04:16:10.211158Z"
    },
    "id": "Pji99t_6gnWm",
    "outputId": "c631adb4-32ef-424a-f3e0-cca7877205e4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install pytorch and torch geometric\n",
    "!pip install conda install pyg -c pyg\n",
    "!pip install ogb\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-07T04:17:48.513000Z",
     "iopub.status.busy": "2022-06-07T04:17:48.512640Z",
     "iopub.status.idle": "2022-06-07T04:17:56.571593Z",
     "shell.execute_reply": "2022-06-07T04:17:56.570769Z",
     "shell.execute_reply.started": "2022-06-07T04:17:48.512964Z"
    },
    "id": "FgRGiwoCe7W8"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.utils import negative_sampling, to_networkx\n",
    "from typing import Union, Tuple\n",
    "from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "\n",
    "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set random seed\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-07T04:17:56.574466Z",
     "iopub.status.busy": "2022-06-07T04:17:56.573629Z",
     "iopub.status.idle": "2022-06-07T04:17:56.581521Z",
     "shell.execute_reply": "2022-06-07T04:17:56.580467Z",
     "shell.execute_reply.started": "2022-06-07T04:17:56.574425Z"
    },
    "id": "uKkniPuSgKc4"
   },
   "outputs": [],
   "source": [
    "#Generating Shortest Path Distance Matrix with Maximum Threshold\n",
    "def get_spd_matrix(G, S, max_spd=5):\n",
    "    spd_matrix = np.zeros((G.number_of_nodes(), len(S)), dtype=np.float32)\n",
    "    for i, node_S in enumerate(S):\n",
    "        for node, length in nx.shortest_path_length(G, source=node_S).items():\n",
    "            spd_matrix[node, i] = min(length, max_spd)\n",
    "    return spd_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-07T04:17:56.583408Z",
     "iopub.status.busy": "2022-06-07T04:17:56.582870Z",
     "iopub.status.idle": "2022-06-07T04:17:56.599957Z",
     "shell.execute_reply": "2022-06-07T04:17:56.599184Z",
     "shell.execute_reply.started": "2022-06-07T04:17:56.583372Z"
    },
    "id": "Q8uSe0B0gMha"
   },
   "outputs": [],
   "source": [
    "# Create logger for results\n",
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 5\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 1].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Train: {result[:, 0].max():.2f}')\n",
    "            print(f'Highest Valid: {result[:, 1].max():.2f}')\n",
    "            print(f'Highest AUC: {result[:, 3].max():.2f}')\n",
    "            print(f'Highest AP: {result[:, 4].max():.2f}')\n",
    "            print(f'Final Train: {result[argmax, 0]:.2f}')\n",
    "            print(f'Final Test: {result[argmax, 2]:.2f}')\n",
    "            print(f'Final AUC: {result[argmax, 3]:.2f}')\n",
    "            print(f'Final AP: {result[argmax, 4]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                train1 = r[:, 0].max().item()\n",
    "                valid = r[:, 1].max().item()\n",
    "                train2 = r[r[:, 1].argmax(), 0].item()\n",
    "                test = r[r[:, 1].argmax(), 2].item()\n",
    "                test_auc = r[r[:, 1].argmax(), 3].item()\n",
    "                test_ap = r[r[:, 1].argmax(), 4].item()\n",
    "                best_results.append((train1, valid, train2, test, test_auc, test_ap))\n",
    "\n",
    "            best_result = torch.tensor(best_results)\n",
    "\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 2]\n",
    "            print(f'Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 3]\n",
    "            print(f'Final Test: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 4]\n",
    "            print(f'AUC Test: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 5]\n",
    "            print(f'AP Test: {r.mean():.2f} ± {r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-07T04:17:56.603438Z",
     "iopub.status.busy": "2022-06-07T04:17:56.602799Z",
     "iopub.status.idle": "2022-06-07T04:17:56.619364Z",
     "shell.execute_reply": "2022-06-07T04:17:56.618609Z",
     "shell.execute_reply.started": "2022-06-07T04:17:56.603402Z"
    },
    "id": "R7G4mDDEiQUO"
   },
   "outputs": [],
   "source": [
    "# custom GraphSAGE\n",
    "class SAGEConv(MessagePassing):\n",
    "    r\"\"\"The GraphSAGE operator from the `\"Inductive Representation Learning on\n",
    "    Large Graphs\" <https://arxiv.org/abs/1706.02216>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\mathbf{W}_1 \\mathbf{x}_i + \\mathbf{W}_2 \\cdot\n",
    "        \\mathrm{mean}_{j \\in \\mathcal{N(i)}} \\mathbf{x}_j\n",
    "\n",
    "    Args:\n",
    "        in_channels (int or tuple): Size of each input sample. A tuple\n",
    "            corresponds to the sizes of source and target dimensionalities.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        normalize (bool, optional): If set to :obj:`True`, output features\n",
    "            will be :math:`\\ell_2`-normalized, *i.e.*,\n",
    "            :math:`\\frac{\\mathbf{x}^{\\prime}_i}\n",
    "            {\\| \\mathbf{x}^{\\prime}_i \\|_2}`.\n",
    "            (default: :obj:`False`)\n",
    "        root_weight (bool, optional): If set to :obj:`False`, the layer will\n",
    "            not add transformed root node features to the output.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n",
    "                 out_channels: int, normalize: bool = False,\n",
    "                 root_weight: bool = True,\n",
    "                 bias: bool = True, **kwargs):  # yapf: disable\n",
    "        kwargs.setdefault('aggr', 'mean')\n",
    "        super(SAGEConv, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "        self.root_weight = root_weight\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        self.lin_l = Linear(in_channels[0], out_channels, bias=bias)\n",
    "        if self.root_weight:\n",
    "            self.lin_r = Linear(in_channels[1], out_channels, bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        if self.root_weight:\n",
    "            self.lin_r.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        if isinstance(x, Tensor):\n",
    "            x: OptPairTensor = (x, x)\n",
    "\n",
    "        # Node and edge feature dimensionalites need to match.\n",
    "        if isinstance(edge_index, Tensor):\n",
    "            assert edge_attr is not None\n",
    "            assert x[0].size(-1) == edge_attr.size(-1)\n",
    "        elif isinstance(edge_index, SparseTensor):\n",
    "            assert x[0].size(-1) == edge_index.size(-1)\n",
    "\n",
    "        # propagate_type: (x: OptPairTensor, edge_attr: OptTensor)\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\n",
    "        out = self.lin_l(out)\n",
    "\n",
    "        x_r = x[1]\n",
    "        if self.root_weight and x_r is not None:\n",
    "            out += self.lin_r(x_r)\n",
    "\n",
    "        if self.normalize:\n",
    "            out = F.normalize(out, p=2., dim=-1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "        return F.relu(x_j + edge_attr)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-07T04:18:01.381414Z",
     "iopub.status.busy": "2022-06-07T04:18:01.380863Z",
     "iopub.status.idle": "2022-06-07T04:18:01.396664Z",
     "shell.execute_reply": "2022-06-07T04:18:01.395647Z",
     "shell.execute_reply.started": "2022-06-07T04:18:01.381369Z"
    },
    "id": "Y2L4rd6uiUNH"
   },
   "outputs": [],
   "source": [
    "# Customer GraphSAGE\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout):\n",
    "        super(GraphSAGE,self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t, edge_attr, emb_ea):\n",
    "        edge_attr = torch.mm(edge_attr, emb_ea)\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, adj_t, edge_attr)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t, edge_attr)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-07T04:18:04.832877Z",
     "iopub.status.busy": "2022-06-07T04:18:04.832198Z",
     "iopub.status.idle": "2022-06-07T04:18:04.842751Z",
     "shell.execute_reply": "2022-06-07T04:18:04.842050Z",
     "shell.execute_reply.started": "2022-06-07T04:18:04.832832Z"
    },
    "id": "L0h1aZNMibP1"
   },
   "outputs": [],
   "source": [
    "# Create link predictor for link prediction\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        x = x_i * x_j\n",
    "        for lin in self.lins[:-1]:\n",
    "            x = lin(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return torch.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-07T04:18:09.942761Z",
     "iopub.status.busy": "2022-06-07T04:18:09.942222Z",
     "iopub.status.idle": "2022-06-07T04:18:09.953020Z",
     "shell.execute_reply": "2022-06-07T04:18:09.951950Z",
     "shell.execute_reply.started": "2022-06-07T04:18:09.942724Z"
    },
    "id": "49J0VK0Fic6t"
   },
   "outputs": [],
   "source": [
    "#Train model\n",
    "def train(model, predictor, edge_attr, x, emb_ea, adj_t, split_edge, optimizer, batch_size):\n",
    "    edge_index = adj_t\n",
    "\n",
    "    model.train()\n",
    "    predictor.train()\n",
    "\n",
    "    pos_train_edge = split_edge['train']['edge'].to(x.device)\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "    for perm in DataLoader(range(pos_train_edge.size(0)), batch_size, shuffle=True):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        h = model(x, adj_t, edge_attr, emb_ea)\n",
    "\n",
    "        edge = pos_train_edge[perm].t()\n",
    "\n",
    "        pos_out = predictor(h[edge[0]], h[edge[1]])\n",
    "        pos_loss = -torch.log(pos_out + 1e-15).mean()\n",
    "\n",
    "        edge = negative_sampling(edge_index, num_nodes=x.size(0),\n",
    "                                 num_neg_samples=perm.size(0), method='dense')\n",
    "\n",
    "        neg_out = predictor(h[edge[0]], h[edge[1]])\n",
    "        neg_loss = -torch.log(1 - neg_out + 1e-15).mean()\n",
    "\n",
    "        loss = pos_loss + neg_loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(x, 1.0)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        num_examples = pos_out.size(0)\n",
    "        total_loss += loss.item() * num_examples\n",
    "        total_examples += num_examples\n",
    "\n",
    "    return total_loss / total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-07T06:01:42.561192Z",
     "iopub.status.busy": "2022-06-07T06:01:42.560725Z",
     "iopub.status.idle": "2022-06-07T06:01:42.578300Z",
     "shell.execute_reply": "2022-06-07T06:01:42.577466Z",
     "shell.execute_reply.started": "2022-06-07T06:01:42.561152Z"
    },
    "id": "I5F5tb9yieoe"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, predictor, edge_attr, x, emb_ea, adj_t, split_edge, evaluator, batch_size):\n",
    "    model.eval()\n",
    "    predictor.eval()\n",
    "\n",
    "    h = model(x, adj_t, edge_attr, emb_ea)\n",
    "\n",
    "    pos_valid_edge = split_edge['valid']['edge'].to(x.device)\n",
    "    neg_valid_edge = split_edge['valid']['edge_neg'].to(x.device)\n",
    "    pos_test_edge = split_edge['test']['edge'].to(x.device)\n",
    "    neg_test_edge = split_edge['test']['edge_neg'].to(x.device)\n",
    "\n",
    "    pos_valid_preds = []\n",
    "    for perm in DataLoader(range(pos_valid_edge.size(0)), batch_size):\n",
    "        edge = pos_valid_edge[perm].t()\n",
    "        pos_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
    "    pos_valid_pred = torch.cat(pos_valid_preds, dim=0)\n",
    "\n",
    "    neg_valid_preds = []\n",
    "    for perm in DataLoader(range(neg_valid_edge.size(0)), batch_size):\n",
    "        edge = neg_valid_edge[perm].t()\n",
    "        neg_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
    "    neg_valid_pred = torch.cat(neg_valid_preds, dim=0)\n",
    "\n",
    "    pos_test_preds = []\n",
    "    for perm in DataLoader(range(pos_test_edge.size(0)), batch_size):\n",
    "        edge = pos_test_edge[perm].t()\n",
    "        pos_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
    "    pos_test_pred = torch.cat(pos_test_preds, dim=0)\n",
    "\n",
    "    neg_test_preds = []\n",
    "    for perm in DataLoader(range(neg_test_edge.size(0)), batch_size):\n",
    "        edge = neg_test_edge[perm].t()\n",
    "        neg_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
    "    neg_test_pred = torch.cat(neg_test_preds, dim=0)\n",
    "\n",
    "    total_preds = torch.cat((pos_test_pred, neg_test_pred), dim=0)\n",
    "    labels = torch.cat((torch.ones_like(pos_test_pred), torch.zeros_like(neg_test_pred)), dim=0)\n",
    "    auc = roc_auc_score(labels.cpu(),torch.round(total_preds.cpu()))\n",
    "    ap_score = average_precision_score(labels.cpu(),torch.round(total_preds.cpu()))\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for K in [20]:\n",
    "        evaluator.K = K\n",
    "        train_hits = evaluator.eval({\n",
    "            'y_pred_pos': pos_valid_pred,\n",
    "            'y_pred_neg': neg_valid_pred,\n",
    "        })[f'hits@{K}']\n",
    "        valid_hits = evaluator.eval({\n",
    "            'y_pred_pos': pos_valid_pred,\n",
    "            'y_pred_neg': neg_valid_pred,\n",
    "        })[f'hits@{K}']\n",
    "        test_hits = evaluator.eval({\n",
    "            'y_pred_pos': pos_test_pred,\n",
    "            'y_pred_neg': neg_test_pred,\n",
    "        })[f'hits@{K}']\n",
    "\n",
    "        auc_results = {'AUC': auc}[f'AUC']\n",
    "        ap_results = {'AP': ap_score}[f'AP']\n",
    "\n",
    "        results[f'Hits@{K}'] = (train_hits, valid_hits, test_hits,auc_results,ap_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-07T04:19:06.707993Z",
     "iopub.status.busy": "2022-06-07T04:19:06.707634Z",
     "iopub.status.idle": "2022-06-07T04:19:06.713812Z",
     "shell.execute_reply": "2022-06-07T04:19:06.712770Z",
     "shell.execute_reply.started": "2022-06-07T04:19:06.707963Z"
    },
    "id": "-1Idf_Zfit5p"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "device = 1\n",
    "#device = torch.device(\"mps\")\n",
    "log_steps = 1\n",
    "num_layers = 2\n",
    "hidden_channels = 256\n",
    "dropout = 0.3\n",
    "batch_size = 64 * 1024\n",
    "lr = 0.003\n",
    "epochs = 400\n",
    "eval_steps = 5\n",
    "runs = 10\n",
    "num_samples = 1\n",
    "node_emb = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-07T04:19:21.016341Z",
     "iopub.status.busy": "2022-06-07T04:19:21.015301Z",
     "iopub.status.idle": "2022-06-07T04:19:21.021966Z",
     "shell.execute_reply": "2022-06-07T04:19:21.021101Z",
     "shell.execute_reply.started": "2022-06-07T04:19:21.016289Z"
    },
    "id": "q5WF90cByqcC"
   },
   "outputs": [],
   "source": [
    "# Define function for centrality and similarity\n",
    "\n",
    "def compute_centrality(G, centrality_type):\n",
    "    \"\"\"\n",
    "    Computes the centrality of the nodes in a graph based on the specified type.\n",
    "    \"\"\"\n",
    "    if centrality_type == 'DC':\n",
    "        return nx.degree_centrality(G)\n",
    "    elif centrality_type == 'EC':\n",
    "        return nx.eigenvector_centrality(G, max_iter=1000)\n",
    "    elif centrality_type == 'BC':\n",
    "        return nx.betweenness_centrality(G)\n",
    "    elif centrality_type == 'CC':\n",
    "        return nx.closeness_centrality(G)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown centrality type: {centrality_type}\")\n",
    "\n",
    "def compute_similarity(G, u, v, similarity_type):\n",
    "    \"\"\"\n",
    "    Computes the similarity between two nodes based on the specified type.\n",
    "    \"\"\"\n",
    "    if similarity_type == 'JA':\n",
    "        return next(nx.jaccard_coefficient(G, [(u, v)]), (0, 0, 0))[2]\n",
    "    elif similarity_type == 'AA':\n",
    "        return next(nx.adamic_adar_index(G, [(u, v)]), (0, 0, 0))[2]\n",
    "    elif similarity_type == 'RA':\n",
    "        return next(nx.resource_allocation_index(G, [(u, v)]), (0, 0, 0))[2]\n",
    "    elif similarity_type == 'PA':\n",
    "        return next(nx.preferential_attachment(G, [(u, v)]), (0, 0, 0))[2]\n",
    "    elif similarity_type == 'CN':\n",
    "        return len(list(nx.common_neighbors(G, u, v)))\n",
    "    elif similarity_type == 'Salton':\n",
    "        cn = len(list(nx.common_neighbors(G, u, v)))\n",
    "        degrees = nx.degree(G, [u, v])\n",
    "        try:\n",
    "            return cn / (degrees[u] * degrees[v])**0.5\n",
    "        except ZeroDivisionError:\n",
    "            return 0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown similarity type: {similarity_type}\")\n",
    "\n",
    "def get_hc_features(G, samples_edges, centrality_type, similarity_type):\n",
    "    \"\"\"\n",
    "    Generates features based on user-selected centrality and similarity measures.\n",
    "    \"\"\"\n",
    "    # Compute centrality for all nodes once\n",
    "    centralities = compute_centrality(G, centrality_type)\n",
    "    feats = []\n",
    "    \n",
    "    for u, v in samples_edges:\n",
    "        # Compute similarity for each pair\n",
    "        similarity = compute_similarity(G, u, v, similarity_type)\n",
    "        u_centrality = centralities[u]\n",
    "       # v_centrality = centralities[v] \n",
    "        \n",
    "        # Add the feature vector\n",
    "        feats.append([similarity, u_centrality])\n",
    "        #feats.append([similarity, u_centrality, v_centrality])\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Dataset\n",
    "# For Cora\n",
    "cora_dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "\n",
    "# For CiteSeer\n",
    "citeseer_dataset = Planetoid(root='/tmp/CiteSeer', name='CiteSeer')\n",
    "\n",
    "# For PubMed\n",
    "pubmed_dataset = Planetoid(root='/tmp/PubMed', name='PubMed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name, device):\n",
    "    split_edge = None\n",
    "\n",
    "    if name in ['ogbl-ddi']:\n",
    "        from ogb.linkproppred import PygLinkPropPredDataset\n",
    "        dataset = PygLinkPropPredDataset(name=name)\n",
    "        split_edge = dataset.get_edge_split()  # Get predefined edge splits for link prediction\n",
    "    elif name == 'Cora':\n",
    "        dataset = cora_dataset  \n",
    "    elif name == 'CiteSeer':\n",
    "        dataset = citeseer_dataset  \n",
    "    elif name == 'PubMed':\n",
    "        dataset = pubmed_dataset  \n",
    "        raise ValueError(\"Dataset not supported\")\n",
    "\n",
    "    data = dataset[0]\n",
    "    edge_index = data.edge_index.to(device)\n",
    "    nx_graph = to_networkx(data, to_undirected=True)\n",
    "\n",
    "    # Convert edge_index to a NumPy array if it's not already one\n",
    "    edgenp = edge_index.cpu().numpy()\n",
    "\n",
    "    # Directly reshape edgenp to the desired 2-column format for edges\n",
    "    edges_reshaped = np.reshape(edgenp, (-1, 2), order='F')\n",
    "    \n",
    "    return nx_graph, edges_reshaped, split_edge\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Example usage\n",
    "nx_graph, edges_reshaped, split_edge = load_dataset('ogbl-ddi', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_train = get_hc_features(nx_graph, edges_reshaped, \"BC\", \"JA\")\n",
    "feat_train=np.array(feat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-07T05:48:46.585211Z",
     "iopub.status.busy": "2022-06-07T05:48:46.584649Z",
     "iopub.status.idle": "2022-06-07T05:48:57.501725Z",
     "shell.execute_reply": "2022-06-07T05:48:57.500892Z",
     "shell.execute_reply.started": "2022-06-07T05:48:46.585166Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save it if nedded.\n",
    "df_save = pd.DataFrame(feat_train, columns = ['j_coefficient','u_centrality'])\n",
    "df_save.to_csv(\"jaccard_degree.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select alaph value\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-07T05:49:12.370667Z",
     "iopub.status.busy": "2022-06-07T05:49:12.370312Z",
     "iopub.status.idle": "2022-06-07T05:49:12.454772Z",
     "shell.execute_reply": "2022-06-07T05:49:12.453887Z",
     "shell.execute_reply.started": "2022-06-07T05:49:12.370636Z"
    }
   },
   "outputs": [],
   "source": [
    "edge_attr = feat_train[:,0]*alpha + feat_train[:,1]*alpha\n",
    "edge_attr = np.reshape(edge_attr, (edge_attr.size, 1))\n",
    "edge_attr =  torch.from_numpy(edge_attr).float().to(device)\n",
    "max_attr = torch.max(edge_attr)\n",
    "min_attr = torch.min(edge_attr)\n",
    "edge_attr = (edge_attr - min_attr) / (max_attr - min_attr + 1e-15)\n",
    "edge_index=edge_index.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-07T06:01:59.884173Z",
     "iopub.status.busy": "2022-06-07T06:01:59.883791Z",
     "iopub.status.idle": "2022-06-07T15:53:44.452713Z",
     "shell.execute_reply": "2022-06-07T15:53:44.451603Z",
     "shell.execute_reply.started": "2022-06-07T06:01:59.884140Z"
    },
    "id": "fpxtbzfzihJy",
    "outputId": "357de10a-cd01-4964-e854-7c02781f0e9c"
   },
   "outputs": [],
   "source": [
    "model = GraphSAGE(node_emb, hidden_channels, hidden_channels,num_layers,dropout).to(device)\n",
    "\n",
    "emb = torch.nn.Embedding(data.num_nodes, node_emb).to(device)\n",
    "emb_ea = torch.nn.Embedding(num_samples, node_emb).to(device)\n",
    "predictor = LinkPredictor(hidden_channels, hidden_channels, 1,\n",
    "                          num_layers+1, dropout).to(device)\n",
    "\n",
    "print('Number of parameters:',\n",
    "      sum(p.numel() for p in list(model.parameters()) +\n",
    "      list(predictor.parameters()) + list(emb.parameters()) + list(emb_ea.parameters())))\n",
    "\n",
    "# encode distance information\n",
    "\n",
    "evaluator = Evaluator(name='ogbl-ddi')\n",
    "loggers = {\n",
    "        'Hits@20': Logger(runs)}\n",
    "\n",
    "for run in range(runs):\n",
    "    random.seed(run)\n",
    "    torch.manual_seed(run)\n",
    "    torch.nn.init.xavier_uniform_(emb.weight)\n",
    "    torch.nn.init.xavier_uniform_(emb_ea.weight)\n",
    "    model.reset_parameters()\n",
    "    predictor.reset_parameters()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(model.parameters()) + list(emb.parameters()) +\n",
    "        list(emb_ea.parameters()) + list(predictor.parameters()), lr=lr)\n",
    "\n",
    "    for epoch in range(1, 1 + epochs):\n",
    "        loss = train(model, predictor, edge_attr, emb.weight, emb_ea.weight, edge_index, split_edge,\n",
    "                      optimizer, batch_size)\n",
    "\n",
    "        if epoch % eval_steps == 0:\n",
    "            results = test(model, predictor, edge_attr, emb.weight, emb_ea.weight, edge_index, split_edge,\n",
    "                            evaluator, batch_size)\n",
    "            for key, result in results.items():\n",
    "                loggers[key].add_result(run, result)\n",
    "\n",
    "            if epoch % log_steps == 0:\n",
    "                for key, result in results.items():\n",
    "                    train_hits, valid_hits, test_hits, auc, ap_score = result\n",
    "                    print(key)\n",
    "                    print(f'Run: {run + 1:02d}, '\n",
    "                          f'Epoch: {epoch:02d}, '\n",
    "                          f'Loss: {loss:.4f}, '\n",
    "                          f'AUC: {100 * auc:.2f}%, '\n",
    "                          f'AP: {100 * ap_score:.2f}%, '\n",
    "                          f'Train: {100 * train_hits:.2f}%, '\n",
    "                          f'Valid: {100 * valid_hits:.2f}%, '\n",
    "                          f'Test: {100 * test_hits:.2f}%')\n",
    "                print('---')\n",
    "\n",
    "    for key in loggers.keys():\n",
    "        print(key)\n",
    "        loggers[key].print_statistics(run)\n",
    "\n",
    "for key in loggers.keys():\n",
    "    print(key)\n",
    "    loggers[key].print_statistics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
